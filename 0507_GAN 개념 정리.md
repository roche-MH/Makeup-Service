## GAN(Generative Adversarial Networks) : 생성적 대립/적대 신경망

- 두개의 네트워크로 구성된 심층 신경망 구조. 

- Generator와 Discriminator가 서로 대립(Adversarial)하며 서로의 성능을 점차 개선해 나간다.

- 어떠한 분포의 데이터도 모방하는 학습 가능.

- 이미지, 음악, 연설, 산문 등의 모든 분야에서 실제 우리의 세계와 유사한 새로운 세계를 창조하도록 학습될 수 있다. 

출처 : https://pathmind.com/kr/wiki/generative-adversarial-network-gan



### 1. Generative Algorithms

↔ Discriminative Algorithms

| Generative                      | Discriminative                |
| ------------------------------- | ----------------------------- |
| 특정 레이블에서 피쳐를 예측     | 특정 피쳐로부터 레이블을 예측 |
| 각 클래스의 분포를 모델링 한다. | 클래스 간의 경계를 학습한다.  |
| `x`를 얻는 방법에 집중          | `y`와` x`의 상관관계에 집중   |



ex) 

이메일의 모든 단어를 판별하여 메세지의 스팸(spam) 인지 스팸이 아닌지(not_spam)를 예측 할 때, 

- spam은 하나의 레이블`y`이고 

- 이메일에서 수집 된 BoW(Bag of Words)는 입력 데이터를 구성하는 feature`x`

**Discriminative** : ` P(y|x)` = BoW가 주어졌을 때, 그 메일이 스팸일 확률

**Generative** : `P(x|y)` = 주어진 레이블 또는 카테고리에 대한 피쳐일 확률

==> 판별 알고리즘은 주로 입력데이터를 분류하는 것에 집중하는 반면, 생성 알고리즘은 데이터를 단순히 분류하는 것 이상의 기능을 수행한다.



### 2. GAN 작동 원리

- generator : 새로운 데이터 인스턴스 생성
  - 목표 : discriminator가 진짜로 판별하게 만드는 인스턴스를 생성하는 것.
  - discrominator로부터 피드백을 받는다.
- discriminator : 데이터의 진위 평가. 데이터 인스턴스가 실제 트레이닝 데이터 세트인지 아닌지 판단한다.
  - 목표 : generator로 부터 전달된 이미지를 가짜로 식별.
  - 이미지 정답 값으로부터 피드백을 받는다.



**동작 단계** (이중 피드백 루프)

1. generator가 임의의 수를 입력받아 생성한 이미지로 반환
2. 생성된 이미지는 실제 데이터 세트에서 가져온 이미지들과 함께 discriminator에 전달
3. discriminator는 실제 이미지와 가짜 이미지를 판별하여 0과 1사이의 확률값으로 반환. (1 = 실제이미지, 0 = 가짜이미지)

> Ian Goodfellow의 “Generative Adversarial Network(2014)” 논문에서 GANs을 지폐위조범과 경찰에 비유. 
>
> 지폐위조범(generator)은 더욱 교묘하게 속이려고 하고 경찰(discriminator)은 이렇게 위조된 지폐를 감별(classify)하려고 한다. 때문에 양쪽 모두 점진적으로 변화하여 결국 두 그룹 모두 속이고 감별하는 서로의 능력이 발전하게 된다.



### 3. GAN 트레이닝 팁

- discriminator를 트레이닝할 때 generator의 파라미터 값을 고정 시키고, generator를 트레이닝할 때는 discriminator의 파라미터 값을 고정시킨다.

  각각 고정된 상대 네트워크의 결과값을 통해 학습한다면, generator가 반드시 학습해야 하는 gradient를 더 잘 반영하게 된다.

- generator 트레이닝 시작 전, discriminator를 실제 데이터를 통해 미리 학습시키면 명확한 gradient값을 얻을 수 있다.



GAN의 두 개의 네트워크를 트레이닝 하다 보면 다음과 같은 문제가 생길 수 있다. 

- discriminator가 너무 뛰어나면 0이나 1에 매우 가까운 gradient값을 반환하게 되어, generator가 gradient값을 제대로 반영하기 어렵게 된다. 

- generator가 너무 뛰어나면 discriminator가 진짜 데이터를 가짜 데이터로 판단할 확률이 높아진다. 

  ==> 이러한 문제는 두 신경망의 학습률(learning rates)을 각각 설정하여 완화할 수 있다. 두 개의 신경망은 항상 비슷한 “학습 수준” 을 유지해야 한다.